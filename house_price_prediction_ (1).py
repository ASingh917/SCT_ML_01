# -*- coding: utf-8 -*-
"""House price prediction .ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1M-3mz8KwjsIpY58woPkHe4Z8UK4efQPo
"""

import pandas as pd

# Load the training data
train_data_path = 'train.csv'
train_data = pd.read_csv(train_data_path)

# Display the first few rows of the dataset to understand its structure
train_data.head(), train_data.info()

from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV
from sklearn.linear_model import LinearRegression, Ridge, Lasso
from sklearn.metrics import mean_absolute_error
from sklearn.preprocessing import StandardScaler, PolynomialFeatures
import numpy as np
import pandas as pd

# Assuming train_data is your DataFrame
# Selecting relevant features
features = ['GrLivArea', 'BedroomAbvGr', 'FullBath']
target = 'SalePrice'

# Drop rows with any missing values in selected columns
df = train_data[features + [target]].dropna()

# Log transformation of the target variable
df[target] = np.log1p(df[target])

# Define X and y
X = df[features]
y = df[target]

# Feature scaling
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)

# Adding polynomial features
poly = PolynomialFeatures(degree=2, include_bias=False)
X_poly = poly.fit_transform(X_scaled)

# Split the data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X_poly, y, test_size=0.2, random_state=42)

# Initialize and train the linear regression model
model = LinearRegression()
model.fit(X_train, y_train)

# Predict on the test set
y_pred = model.predict(X_test)

# Convert predictions back to the original scale
y_pred_original_scale = np.expm1(y_pred)
y_test_original_scale = np.expm1(y_test)

# Calculate and print the mean absolute error
mae = mean_absolute_error(y_test_original_scale, y_pred_original_scale)
print(f'Mean Absolute Error: {mae:.2f}')

# Calculate R^2 score
r_squared = model.score(X_test, y_test)
print(f'R-squared: {r_squared:.2f}')

# Cross-validation to check model performance
cv_scores = cross_val_score(model, X_poly, y, cv=5, scoring='neg_mean_absolute_error')
print(f'Cross-Validation MAE: {-np.mean(cv_scores):.2f}')

# Regularization (Ridge and Lasso) with hyperparameter tuning
ridge = Ridge()
lasso = Lasso()
ridge_params = {'alpha': [0.1, 1, 10]}
lasso_params = {'alpha': [0.001, 0.01, 0.1]}

# Grid search for Ridge regression
ridge_grid = GridSearchCV(ridge, ridge_params, cv=5, scoring='neg_mean_absolute_error')
ridge_grid.fit(X_train, y_train)
print(f'Best Ridge MAE: {-ridge_grid.best_score_:.2f} with alpha: {ridge_grid.best_params_}')

# Grid search for Lasso regression
lasso_grid = GridSearchCV(lasso, lasso_params, cv=5, scoring='neg_mean_absolute_error')
lasso_grid.fit(X_train, y_train)
print(f'Best Lasso MAE: {-lasso_grid.best_score_:.2f} with alpha: {lasso_grid.best_params_}')

mae

import matplotlib.pyplot as plt
import seaborn as sns

# Plot 1: Predicted vs. Actual Prices
plt.figure(figsize=(10, 6))
plt.scatter(y_test, y_pred, color='blue', edgecolor='k', alpha=0.7)
plt.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'k--', lw=3)
plt.xlabel('Actual SalePrice')
plt.ylabel('Predicted SalePrice')
plt.title('Predicted vs. Actual House Prices')
plt.show()